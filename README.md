# GCP 

This is a project which shows how we can use Pytorch lightning for distributed training 
Key highlights

1. Hydra for config managment -> It helps keep track of our hyperparameters.
2. GCP infrastructure to develop instance group for distributed training.
3. Pytorch Lightning -> For distriubted training on the infrastructure created on GCP
4. The code has been developed in modular fashion
5. Open source tool used-> MLflow for experminet tracking and logging, docker for conterization of code

