# GCP

This is a project that shows how we can use Pytorch lightning for distributed training
Key highlights

1. Hydra for config management -> It helps keep track of our hyperparameters.
2. GCP infrastructure to develop instance groups for distributed training.
3. Pytorch Lightning -> For distributed training on the infrastructure created on GCP
4. The code has been developed in a modular fashion
5. Open source tool used-> MLflow for experiment tracking and logging, docker for containerization of code
